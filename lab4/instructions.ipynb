{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Model-Based Offline Optimization\n",
    "\n",
    "In this lab, we explore **Model-Based Offline Optimization (MBO)**, a method to optimize black-box functions using only a pre-collected dataset, without additional function evaluations. We'll use two functions from the **Black-Box Optimization Benchmarking (BBOB)** suite:\n",
    "- **f1** [Sphere Function](https://numbbo.github.io/gforge/downloads/download16.00/bbobdocfunctions.pdf#page=5): A simple, unimodal function with a global minimum at (0, 0).\n",
    "- **f22** [Gallagher's Gaussian 21-hi Peaks Function](https://numbbo.github.io/gforge/downloads/download16.00/bbobdocfunctions.pdf#page=110): A complex, multimodal function with many local optima.\n",
    "\n",
    "Visualizations of these problems are available [here](https://coco-platform.org/testsuites/bbob/viz.html?col=3&dim=2&fun=1&ins=1&typ=all).\n",
    "\n",
    "### Objectives\n",
    "1. **Dataset:** Use pre-collected datasets (100k samples each) for f1 and f22, generated via **Latin Hypercube Sampling (LHS)** [learn more about LHS here](https://en.wikipedia.org/wiki/Latin_hypercube_sampling).\n",
    "2. **Surrogate Model:** Train a Multi-Layer Perceptron (MLP) to approximate these functions.\n",
    "3. **Visualization:** Compare the MLP's predictions to the true functions using contour and scatter plots.\n",
    "4. **Optimization:** Implement gradient-based optimization on the MLP to find optimal points.\n",
    "5. **Experiments:** Investigate how training dataset size and model complexity affect performance.\n",
    "\n",
    "### Key Concepts\n",
    "- **Offline Optimization:** No new evaluations of the true function are allowed; we rely solely on the dataset.\n",
    "- **Surrogate Model:** The MLP acts as a cheap, differentiable proxy for the expensive black-box function.\n",
    "- **Gradient-Based Optimization:** Use the MLP's gradients to find optimal designs efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preparation\n",
    "\n",
    "We start by loading and splitting the dataset into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloaders(\n",
    "    file_path: str,\n",
    "    batch_size: int = 256,\n",
    "    train_perc: float = 0.6,\n",
    "    val_perc: float = 0.2,\n",
    ") -> tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Prepare train, validation and test dataloaders.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to CSV data file\n",
    "        batch_size: Batch size for dataloaders\n",
    "        train_perc: Percentage of data for training\n",
    "        val_perc: Percentage of data for validation\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    data_df = pd.read_csv(file_path)\n",
    "    X = torch.tensor(data_df[[\"x1\", \"x2\"]].values, dtype=torch.float32)\n",
    "    y = torch.tensor(data_df[\"y\"].values.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "    # Split data\n",
    "    num_samples = len(X)\n",
    "    indices = np.random.permutation(num_samples)\n",
    "    train_size = int(train_perc * num_samples)\n",
    "    val_size = int(val_perc * num_samples)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = TensorDataset(X[indices[:train_size]], y[indices[:train_size]])\n",
    "    val_dataset = TensorDataset(\n",
    "        X[indices[train_size : train_size + val_size]],\n",
    "        y[indices[train_size : train_size + val_size]],\n",
    "    )\n",
    "    test_dataset = TensorDataset(\n",
    "        X[indices[train_size + val_size :]], y[indices[train_size + val_size :]]\n",
    "    )\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader\n",
    "\n",
    "# Example usage (replace with your file path)\n",
    "train_dataloader, val_dataloader, test_dataloader = prepare_dataloaders(\n",
    "    \"./bbob_f022_i01_d02_samples.csv\", train_perc=0.3, val_perc=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define the MLP Model\n",
    "\n",
    "The MLP will serve as our surrogate model to approximate the black-box function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim: int = 2, hidden_dim: int = 256, n_layers: int = 3\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Multi-Layer Perceptron (MLP) for function approximation.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Number of input features (e.g., 2 for x1, x2).\n",
    "            hidden_dim (int): Number of neurons in each hidden layer.\n",
    "            n_layers (int): Total number of layers (input + hidden + output).\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(n_layers - 2):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        # Output layer -> 1 output neuron (regression, single-objective problem)\n",
    "        layers.append(nn.Linear(hidden_dim, 1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training the Model\n",
    "\n",
    "We train the MLP using the Adam optimizer and Mean Squared Error (MSE) loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, targets in train_dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    train_loss = train_loss / len(train_dataloader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "    val_loss = val_loss / len(val_dataloader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "test_loss = test_loss / len(test_dataloader.dataset)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualization\n",
    "\n",
    "Let's visualize how well the MLP approximates the true function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contour_comparison(\n",
    "    model: nn.Module, test_dataloader: DataLoader, device: torch.device\n",
    ") -> None:\n",
    "    test_inputs_list = []\n",
    "    test_targets_list = []\n",
    "\n",
    "    for inputs, targets in test_dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        test_inputs_list.append(inputs.cpu().numpy())\n",
    "        test_targets_list.append(targets.cpu().numpy())\n",
    "\n",
    "    test_inputs = np.vstack(test_inputs_list)\n",
    "    test_targets = np.concatenate(test_targets_list)\n",
    "\n",
    "    x1_min, x1_max = test_inputs[:, 0].min(), test_inputs[:, 0].max()\n",
    "    x2_min, x2_max = test_inputs[:, 1].min(), test_inputs[:, 1].max()\n",
    "    x1_vals = np.linspace(x1_min, x1_max, 100)\n",
    "    x2_vals = np.linspace(x2_min, x2_max, 100)\n",
    "    X1, X2 = np.meshgrid(x1_vals, x2_vals)\n",
    "\n",
    "    grid_points = np.column_stack((X1.flatten(), X2.flatten()))\n",
    "    grid_tensor = torch.tensor(grid_points, dtype=torch.float32, device=device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(grid_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    Z_pred = predictions.reshape(X1.shape)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    true_contour = ax1.tricontourf(\n",
    "        test_inputs[:, 0], test_inputs[:, 1], test_targets.reshape(-1)\n",
    "    )\n",
    "    ax1.set_title(\"Test Data: True Values\")\n",
    "    ax1.set_xlabel(\"x1\")\n",
    "    ax1.set_ylabel(\"x2\")\n",
    "    plt.colorbar(true_contour, ax=ax1)\n",
    "\n",
    "    pred_contour = ax2.contourf(X1, X2, Z_pred)\n",
    "    ax2.set_title(\"Model: Predicted Values\")\n",
    "    ax2.set_xlabel(\"x1\")\n",
    "    ax2.set_ylabel(\"x2\")\n",
    "    plt.colorbar(pred_contour, ax=ax2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_contour_comparison(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_scatter(\n",
    "    model: nn.Module, test_dataloader: DataLoader, device: torch.device\n",
    ") -> None:\n",
    "    test_targets_list = []\n",
    "    test_preds_list = []\n",
    "\n",
    "    for inputs, targets in test_dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "\n",
    "        test_targets_list.append(targets.cpu().numpy())\n",
    "        test_preds_list.append(outputs.cpu().numpy())\n",
    "\n",
    "    test_targets = np.concatenate(test_targets_list)\n",
    "    test_preds = np.concatenate(test_preds_list).flatten()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.scatter(test_targets, test_preds, alpha=0.5)\n",
    "    ax.plot(\n",
    "        [test_targets.min(), test_targets.max()],\n",
    "        [test_targets.min(), test_targets.max()],\n",
    "        \"r--\",\n",
    "        label=\"Perfect prediction\",\n",
    "    )\n",
    "    ax.set_xlabel(\"True values\")\n",
    "    ax.set_ylabel(\"Predicted values\")\n",
    "    ax.set_title(\"Test Set: Predictions vs True Values\")\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_prediction_scatter(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Gradient-Based Optimization\n",
    "\n",
    "Now, we optimize the trained MLP to find the input that minimizes its output (our surrogate's prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(\n",
    "    model: nn.Module, num_steps: int = 1000\n",
    ") -> tuple[torch.Tensor, float]:\n",
    "    \"\"\"\n",
    "    Perform gradient-based optimization on the MLP.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained MLP model.\n",
    "        num_steps (int): Number of optimization steps.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best_x, best_y) - Optimal input and its predicted output.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # Select random starting point in [-5, 5]^2\n",
    "    x = torch.rand(1, 2, device=device) * 10 - 5\n",
    "    x.requires_grad_(True)\n",
    "    raise NotImplementedError(\"Not implemented\")\n",
    "    optimizer = None # TODO: select optimizer\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        optimizer.zero_grad()\n",
    "        # Predict output\n",
    "        # Compute gradients\n",
    "        # Update x\n",
    "        # Enforce domain constraints [-5, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Impact of Dataset Size\n",
    "- **Task:** Train the MLP with different training set sizes (e.g., 100, 1000, 5000, 10000, 50000).\n",
    "- **Instructions:**\n",
    "  1. Modify the `train_perc` parameter in `prepare_dataloaders` to achieve these different dataset sizes (keep the test set size fixed at approximately 30,000 samples).\n",
    "  2. Train the model for each dataset size and generate corresponding contour plots.\n",
    "  3. Analyze how the MLP's ability to capture the underlying function structure (particularly the multimodal nature of f22) improves with increasing data volume.\n",
    "  4. Select and justify a single quantitative metric that effectively captures the model’s predictive quality in this context. Use this metric to evaluate model performance across different training set sizes. Present the results as a plot to visualize how model quality scales with data availability.\n",
    "\n",
    "### Exercise 2: Optimization Robustness\n",
    "- **Task:** Implement the `optimize_model` function to find inputs that minimize the model's output.\n",
    "- **Instructions:**\n",
    "  1. Execute the optimization process multiple times (e.g. 10 for each dataset) with different random starting points and record the best surrogate values.\n",
    "  2. Create a histogram of the best `y` values across all optimization runs to visualize the distribution of results.\n",
    "  3. Generate a contour plot showing the locations of the solutions found. Analyze the consistency of these solutions.\n",
    "  4. Use `cocoex` to evaluate the true objective function at the solution points. Compare these values against the surrogate’s predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Original COCO Benchmark Problems\n",
    "\n",
    "The code below installs and imports the necessary packages to access the original COCO benchmark problems (f1 and f22).\n",
    "This will allow us to compare our model's predictions with the true functions and evaluate optimization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install coco-experiment cocopp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cocoex import Suite\n",
    "\n",
    "function_id = 22  # Switch between 1 and 22 to change the function\n",
    "\n",
    "suite = Suite(\n",
    "    \"bbob\",\n",
    "    \"\",\n",
    "    f\"function_indices: {function_id}, dimensions: 2 instance_indices: 1\",\n",
    ")\n",
    "for function in suite:\n",
    "    print(function.info)\n",
    "    print(\"Function value at (0, 0):\", function(np.array([0, 0])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
